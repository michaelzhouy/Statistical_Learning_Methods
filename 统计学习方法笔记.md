# 第1章统计学习方法概论
---
## 1.1 统计学习
1. 统计学习的对象：数据
2. 基本假设：同类数据具有一定的统计规律性，这是统计学习的前提。同类数据：具有某种共同属性的数据
3. 统计学习的目的：对数据（特别是对未知数据）进行预测与分析
4. 统计学习的分类：监督学习，非监督学习，半监督学习，强化学习
5. 实现统计学习方法的步骤：
    1. 得到一个有限的训练数据集合
    2. 确定模型的**假设空间**，即模型的集合
    3. 确定模型选择的标准，即**学习策略**
    4. 求解最优模型的算法，即**学习算法**
    5. 通过学习方法选择最优模型
    6. 利用学习的最优模型对新数据进行预测或分析

---
## 1.2 监督学习
1. 训练集：
$$
T = \{(x_1, y_1), (x_2, y_2), ..., (x_N,y_N)\}
$$
2. 实例x的特征向量
$$
x=(x^{(1)},x^{(2)},...,x^{(n)})^T
$$
3. 模型：
    1. 决策模型 
    $$
    Y=f(X)
    $$
    预测模型 
    $$
    y=f(x)
    $$
    2. 条件概率分布 
    $$
    P(Y|X)
    $$
    预测形式
    $$
    \mathop{\arg\max}\limits_{y}(P(y|x))
    $$
2. 监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。学习的目的在于找到最好的模型
3. 假设空间=模型的集合
4. 模型：输入空间到输出空间的映射关系
5. 监督学习的模型：
- 概率模型：条件概率分布P(Y|X)
- 非概率模型：决策函数Y=f(X)

---
## 1.3 统计学习三要素：方法=模型+策略+算法
### 1.3.1 模型（假设空间）
1. 决策函数
$$
F=\{f|Y=f_{\theta}(X),\theta \in R^n\}
$$
2. 条件概率分布
$$
F=\{P|P_{\theta}(Y|X),\theta \in R^n\}
$$
3. 模型的假设空间包含所有可能的条件概率分布或决策函数。比如：假设决策函数是输入变量的线性函数，那么模型的假设空间就是所有这些线性函数构成的函数集合
4. 决策函数表示模型为非概率模型
5. 条件概率表示模型为概率模型

### 1.3.2 策略（损失函数）
1. **损失函数**度量模型一次预测的好坏
2. **风险函数**度量平均意义下模型预测的好坏
3. 损失函数
- 0-1损失函数：
$$
L(Y,f(X))=\begin{cases}1,\text{Y!=f(X)}\\0,\text{Y=f(X)}\end{cases}
$$
- 平方损失函数
$$
L(Y,f(X))=(Y-f(X))^2
$$
- 绝对损失函数
$$
L(Y,f(X))=|Y-f(X)|
$$
- 对数损失函数
$$
L(Y,P(Y|X))=-log P(Y|X)
$$
4. 经验风险最小化
$$
\min_{f \in F} \frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))
$$
5. 结构风险最小化
$$
\min_{f\in F}\frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))+\lambda J(f)
$$
4. 损失函数的期望被称为风险函数或期望损失或期望风险
5. 经验风险或经验损失：训练数据集的平均损失
6. 当样本容量N趋于无穷时，经验风险趋于期望风险
7. 用经验风险来估计期望风险
8. 经验风险最小化与结构风险最小化
- 经验风险最小化的策略认为经验风险最小的模型是最优模型
- 结构风险最小化的策略认为结构风险最小的模型是最优模型
- 结构风险=经验风险+正则化项

### 1.3.3 算法
1. 算法是指学习模型的具体计算方法

---
## 1.4 模型评估与模型选择
1. 训练误差
$$
\frac{1}{N}\sum_{i=1}^N L(y_i,\hat f(x_i))
$$
2. 测试误差
$$
\frac{1}{N^{'}}\sum_{i=1}^{N^{'}} L(y_i,\hat f(x_i))
$$

---
## 1.5 正则化与交叉验证
1. 最小化结构风险
$$
\frac1N\sum_{i=1}^N L(y_i,f(x_i))+\lambda J(f)
$$
2. 交叉验证：随机地将数据集划分为训练集、验证集和测试集

---
## 1.6 泛化能力

---
## 1.7 生成模型与判别模型
1. 生成模型：由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)=P(X,Y)/P(X)做出预测的模型
$$
P(Y|X)=\frac{P(X,Y)}{P(X)}
$$
2. 判别模型
$$
f(X)或P(Y|X)
$$

---
## 1.8 分类问题
1. TP：True Positive 将正类预测为正类的个数
2. FN：False Negetive 将正类预测为负类的个数
3. FP：False Positive 将负类预测为正类的个数
4. TN：True Negetive 将负类预测为负类的个数
5. 精准率
$$
P=\frac{TP}{TP|FP}
$$
6. 召回率
$$
R=\frac{TP}{TP+FN}
$$

---
# 第2章 感知机
---
## 2.1 感知机模型
1. 输入空间
$$
X\subseteq R^n
$$
2. 输入变量
$$
x\in X
$$
3. 输出空间
$$
Y=\{+1,-1\}
$$
4. 输出变量
$$
y \in \{+1,-1\} 
$$
5. 假设空间
$$
f(x)=sign(w·x+b)
$$
---
## 2.2 感知机学习策略
1. 损失函数：误分类点到超平面的总距离
$$
L(w, b) = -\sum_{x_i\in M}y_i(w·x_i+b)
$$

---
## 2.3 感知机学习算法
1. 感知机学习算法的原始形式

输入：训练数据集T，学习率η
$$
T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$

- 选取初值w0，b0
- 在训练数据集中选取数据(xi,yi)
- 如果$y_i(w·x_i+b)<=0$

$w:=w+\eta y_i x_i$
$b:=b+\eta y_i$
- 转至2，直至训练集中没有误分类的点

输出：w，b
2. 感知机模型对偶形式
$$
f(x)=sign\left(\sum_{i=1}^N \alpha_j y_j x_j·x+b\right)
\\\alpha=(\alpha_1,...,\alpha_N)^T
$$

算法：

输入：训练数据集T，学习率η


输出：α，b

    1. 初值α:=0，b:=0
    2. 在训练集中选取数据(xi,yi)
    3. 如果
$$
y_i\left(\sum_{j=1}^N\alpha_j y_j x_j ·x+b\right)<=0
\\\Rightarrow
\alpha_i :=\alpha_i+\eta
\\b:=b+\eta y_i
$$
    4. 转至2，直至训练集中没有误分类点

---
# 第3章 k近邻法

---
## 3.1 k近邻算法
1. 输入：训练数据集T
$$
T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
\\其中x_i\in X \subseteq R^n
\\y_i\in Y=\{c_1,c_2,...,c_K\}
$$
2. 输出：实例x所属的类别y
3. 根据给定的**距离度量**，在训练集中找到与x最近的k个点，涵盖这k个点的邻域记作
$$
N_k(x)
$$
4. 在$N_k(x)$中根据分类决策规则（如多数表决）决定x的类别y

## 3.2 k近邻模型
1. 距离度量：欧式距离
2. k值的选择：k值太小→过拟合，k值太大→模型简单，欠拟合
3. 分类决策规则：多数投票法


## 3.3 k近邻法的实现：kd树

---
# 第4章 朴素贝叶斯法
1. 模型的分类
- 决策模型 Y=f(X)
- 条件概率 P(Y|X)

- 判别模型 Y=f(X) P(Y|X)
- 生成模型 P(Y|X)=P(X,y)/P(X)

## 4.1 朴素贝叶斯法的学习和分类
1. 生成模型
$$
P(Y=c_k|X=x)=\frac{P(Y=c_k)·P(X=x|Y=c_k)}{P(X=x)}
$$
2. 模型假设，条件独立
$$
P(X=x|Y=c_k)=\prod_{i=1}^n P(X^{(i)}=x^{(i)}|Y=c_k)
$$
3. 预测准则：后验概率最大
$$
y=\mathop{\arg\max}\limits_{c_k}P(Y=c_k|X=x)
$$

## 4.2 朴素贝叶斯法的参数估计
1. 用训练集实例估计
$$
P(Y=c_k)
\\P(X=x|Y=c_k)
$$
2. 极大似然估计
$$
P(Y=c_1)=\frac{\#\{y_i=c_1\}}{N}
\\P(Y=c_2)=\frac{\#\{y_i=c_2\}}{N}

P(X^{(1)}=x^{(1)}|Y=c_1)=\frac{\#(y_i=c_1,X^{(1)}=x^{(1)})}{\#(y_i=c_1)}
$$
3. 贝叶斯估计
$$
P(Y=c_1)=\frac{\#\{y_i=c_1\}+\lambda}{N+k\lambda}
\\P(Y=c_2)=\frac{\#\{y_i=c_2\}+\lambda}{N+k\lambda}
\\···
\\P(Y=c_k)=\frac{\#\{y_i=c_k\}+\lambda}{N+k\lambda}

\\P(X^{(i)}|Y)=\frac{+\lambda}{+S_i\lambda}
$$
